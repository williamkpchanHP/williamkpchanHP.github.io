<base target="_blank"><html><head><title>RNotes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script type='text/javascript' src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script src="https://d3js.org/d3.v4.min.js"></script>

<script>
  var showTopicNumber = false;
  var topicEnd = "<br>";
  var bookid = "RNotes"
  var markerName = "h2"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
h1, h2 {color: gold;}
strong {color: orange;}
pre{width:100%;}
#toc{color:cyan; font-size:20px;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px; background-color:#044;}
.stop-scrolling { height: 100%; overflow: hidden; }
iframe {width:100%; height:100%;}
</style></head><body onkeypress="chkKey()"><center>

<div id="toc"></div></center>
<pre>
<br>
<br>

<h2>Web Scraping with R and PhantomJS</h2>
<div class="toc" id="PhantomJStoc"><a href="#PhantomJStopic-1" target="_self">Load the necessary packages</a><br><a href="#PhantomJStopic-2" target="_self">Scraping Javascript Generated Data with R</a><br></div></center>

Short tutorial on scraping Javascript generated data with R using PhantomJS.
When you need to do web scraping, you would normally make use of Hadley Wickham’s <code>rvest</code> package.
This package provides an easy to use, out of the box solution to fetch the html code that generates a webpage.
However, when the website or webpage makes use of JavaScript to display the data you're interested in, the <code>rvest</code> package misses the required functionality.
One solution is to make use of PhantomJS.

<h3 id="PhantomJStopic-1">Load the necessary packages</h3>
<code>library(rvest)
library(stringr)
library(plyr)
library(dplyr)
library(ggvis)
library(knitr)
options(digits = 4)</code>

<h3 id="PhantomJStopic-2">Scraping Javascript Generated Data with R</h3>
The next step is the collection of the TechStars data using PhantomJS. Check out the following basic .js file:

<code>// scrape_techstars.js

var webPage = require('webpage');
var page = webPage.create();

var fs = require('fs');
var path = 'techstars.html'

page.open('http://www.techstars.com/companies/stats/', function (status) {
  var content = page.content;
  fs.write(path,content,'w')
  phantom.exit();
});</code>

The script basically renders the HTML page after the underlying javascript code has done its work, allowing you to fetch the HTML page, with all the tables in there. To stay in R for the rest of this analysis, we suggest you use the <code>system()</code> function to invoke PhantomJS (you'll have to <a href="http://phantomjs.org/download.html">download and install PhantomJS</a> and put it in your working directory):

<code># Let phantomJS scrape techstars, output is written to techstars.html
system("./phantomjs scrape_techstars.js")</code>

After this small detour, you finally have an HTML file, <code>techstars.html</code>, on our local system, that can be scrape with <code>rvest</code>. An inspection of the Techstars webpage reveals that the tables we're interested in are located in divs with the css class <code>batch</code>:

<code>batches &lt;- html("techstars.html") %&gt;%
  html_nodes(".batch")

class(batches)</code>

<code>[1] "XMLNodeSet"</code>
You now have a list of <code>XMLNodeSet</code> objects: each object contains the data for a single TechStars batch. 
In there, we can find information concerning the batch location, the year, the season, but also about the companies, their current headquarters, their current status and the amount of funding they raised in total. 
We will not go into detail on the data collection and cleaning steps below; you can execute the code yourself and inspect what they accomplish. 

You'll see that some custom cleaning is going on to make sure that each bit of information is nicely formatted:

<code>batch_titles &lt;- batches %&gt;%
  html_nodes(".batch_class") %&gt;%
  html_text()

batch_season &lt;- str_extract(batch_titles, "(Fall|Spring|Winter|Summer)")
batch_year &lt;- str_extract(batch_titles, "([[:digit:]]{4})")
# location info is everything in the batch title that is not year info or season info
batch_location &lt;- sub("\\s+$", "",
                      sub("([[:digit:]]{4})", "",
                          sub("(Fall|Spring|Winter|Summer)","",batch_titles)))

# create data frame with batch info.
batch_info &lt;- data.frame(location = batch_location,
                         year = batch_year,
                         season = batch_season)

breakdown &lt;- lapply(batches, function(x) {
  company_info &lt;- x %&gt;% html_nodes(".parent")
  companies_single_batch &lt;- lapply(company_info, function(y){
    as.list(gsub("\\[\\+\\]\\[\\-\\]\\s", "", y %&gt;%
       html_nodes("td") %&gt;%
       html_text()))
  })
  df &lt;- data.frame(matrix(unlist(companies_single_batch),
                   nrow=length(companies_single_batch),
                   byrow=T,
                   dimnames = list(NULL, c("company","funding","status","hq"))))
  return(df)
})

# Add batch info to breakdown
batch_info_extended &lt;- batch_info[rep(seq_len(nrow(batch_info)),
                                  sapply(breakdown, nrow)),]
breakdown_merged &lt;- rbind.fill(breakdown)

# Merge all information
techstars &lt;- tbl_df(cbind(breakdown_merged, batch_info_extended)) %&gt;%
  mutate(funding = as.numeric(gsub(",","",gsub("\\$","",funding))))</code>

With a combination of core R, <code>rvest</code>, <code>plyr</code> and <code>dplyr</code> functions, we now we have the <code>techstars</code> data frame; a data set of all TechStars company, with all publicly available information that is nicely formatted:

<code>techstars</code>

<code>## Source: local data frame [535 x 7]
##
##          company funding   status                hq location year season
## 1    Accountable  110000   Active    Fort Worth, TX   Austin 2013   Fall
## 2          Atlas 1180000   Active        Austin, TX   Austin 2013   Fall
## 3        Embrace  110000   Failed        Austin, TX   Austin 2013   Fall
## 4  Filament Labs 1490000   Active        Austin, TX   Austin 2013   Fall
## 5        Fosbury  300000   Active        Austin, TX   Austin 2013   Fall
## 6          Gone!  840000   Active San Francisco, CA   Austin 2013   Fall
## 7     MarketVibe  110000 Acquired        Austin, TX   Austin 2013   Fall
## 8           Plum 1630000   Active        Austin, TX   Austin 2013   Fall
## 9  ProtoExchange  110000   Active        Austin, TX   Austin 2013   Fall
## 10       Testlio 1020000   Active        Austin, TX   Austin 2013   Fall
## ..           ...     ...      ...               ...      ...  ...    ...</code>

<code>names(techstars)</code>

<code>## [1] "company"  "funding"  "status"   "hq"       "location" "year"
## [7] "season"</code>

<h2>start chrome from R console with ChromeDriver and RSelenium</h2>

verbose logging to check server in more detail:
library(wdman)
library(RSelenium)
selServ <- selenium(jvmargs = c("-Dwebdriver.chrome.verboseLogging=true"))
remDr <- remoteDriver(port = 4567L, browserName = "chrome")
remDr$open()
selServ$log()

or if you prefer running seperately in the terminal start a selenium server as follows:

java -Dwebdriver.chrome.verboseLogging=true -Dwebdriver.chrome.driver=/home/hdpusr/ChromeDriver/chromedr‌​‌​iver -jar selenium-server-standalone-3.0.1.jar -port 4444

You can then file an issue with https://bugs.chromium.org/p/chromedriver/issues/list if the problem is not apparent.

<a href="https://www.selenium.dev/" class="whitebut ">Selenium automates browsers</a>

<a href="https://chromedriver.chromium.org/getting-started" class="whitebut ">ChromeDriver for testing website on desktop</a>

https://github.com/rstudio/webdriver
A client for the ‘WebDriver’ ‘API’ only tested with ‘PhantomJS’

<h2>R extrect pdf Title</h2>
assumptions about the structure of the pdf we wish to scrape. 
The code below makes the following assumptions:

Title and abstract are on page 1
Title is of height 15
The abstract is between the first occurrence of the word "Abstract" and first occurrence of the word "Introduction"

options("encoding" = "native.enc")
Sys.setlocale(category = 'LC_ALL', 'Chinese')

datapathOCR = "C:/Users/william/Desktop"
setwd(datapathOCR)

library(tidyverse)
library(pdftools)
#data = pdf_data("~/Desktop/a.pdf")
# ~/Desktop isC:/Users/william/Documents
data = pdf_data("a.pdf")

#Get First page
page_1 = data[[1]]

# title = data[[11]] %>% filter(height == 12) %>% .$text
# title
# writeClipboard(as.character(data[[11]]))

# Get Title, here we assume its of size 15
title = page_1 %>% filter(height == 15) %>% .$text %>% paste0(collapse = " ")

#Get Abstract
abstract_start = which(page_1$text == "Abstract.")[1]
introduction_start = which(page_1$text == "Introduction")[1]

abstract = page_1$text[abstract_start:(introduction_start-2)]%>%
paste0(collapse = " ")

# str(data[[1]])
# tbl_df [18 x 6] (S3: tbl_df/tbl/data.frame)
#  $ width : int [1:18] 183 13 15 9 14  ...
#  $ height: int [1:18] 14 150 45 9 14  ...
#  $ x     : int [1:18] 119 384 57 240 240  ...
#  $ y     : int [1:18] 114 162 401 515 529  ...
#  $ space : logi [1:18] FALSE FALSE FALSE FALSE  ...
#  $ text  : chr [1:18] "\"十二五\"国家重点图书出版规划项目" "中华中医药学会组织编写" ...

# page11 = data[[11]]$text
# page11text = gsub('U001001ba.*','',page11) not work
# data[[11]][10,]

<h2>data visualization techniques</h2>
<div id="toc" class="toc"><a href="#dataVisualizationtopic-0" target="_self">what is data visualization?</a><br><a href="#dataVisualizationtopic-1" target="_self">data visualization techniques</a><br><a href="#dataVisualizationtopic-2" target="_self">1. pie chart</a><br><a href="#dataVisualizationtopic-3" target="_self">2. bar chart</a><br><a href="#dataVisualizationtopic-4" target="_self">3. histogram</a><br><a href="#dataVisualizationtopic-5" target="_self">4. gantt chart</a><br><a href="#dataVisualizationtopic-6" target="_self">5. heat map</a><br><a href="#dataVisualizationtopic-7" target="_self">6. a box and whisker plot</a><br><a href="#dataVisualizationtopic-8" target="_self">7. waterfall chart</a><br><a href="#dataVisualizationtopic-9" target="_self">8. area chart</a><br><a href="#dataVisualizationtopic-10" target="_self">9. scatter plot</a><br><a href="#dataVisualizationtopic-11" target="_self">10. pictogram chart</a><br><a href="#dataVisualizationtopic-12" target="_self">11. timeline</a><br><a href="#dataVisualizationtopic-13" target="_self">12. highlight table</a><br><a href="#dataVisualizationtopic-14" target="_self">13. bullet graph</a><br><a href="#dataVisualizationtopic-15" target="_self">14. choropleth maps</a><br><a href="#dataVisualizationtopic-16" target="_self">15. word cloud</a><br><a href="#dataVisualizationtopic-17" target="_self">16. network diagram</a><br><a href="#dataVisualizationtopic-18" target="_self">17. correlation matrix</a><br><a href="#dataVisualizationtopic-19" target="_self">other data visualization options</a><br><a href="#dataVisualizationtopic-20" target="_self">tips for creating effective visualizations</a><br><a href="#dataVisualizationtopic-21" target="_self">visuals to interpret and share information</a><br></div></center>

<img class="lazy" data-src="https://miro.medium.com/max/875/1*Ar9Y0RB6Dn7BufIabGJk0g.png">

There’s a growing demand for business analytics and data expertise in the workforce. 
But you don’t need to be a professional analyst to benefit from data-related skills.

Becoming skilled at common data visualization techniques can help you reap the rewards of data-driven decision-making, including increased confidence and potential cost savings. 
Learning how to effectively visualize data could be the first step toward using data analytics and data science to your advantage to add value to your organization.

Several data visualization techniques can help you become more effective in your role. 
Here are 17 essential data visualization techniques all professionals should know, as well as tips to help you effectively present your data.
<h3 id="dataVisualizationtopic-0">what is data visualization?</h3>
Data visualization is the process of creating graphical representations of information. 
This process helps the presenter communicate data in a way that’s easy for the viewer to interpret and draw conclusions.

There are many different techniques and tools you can leverage to visualize data, so you want to know which ones to use and when. 
Here are some of the most important data visualization techniques all professionals should know.
<h3 id="dataVisualizationtopic-1">data visualization techniques</h3>
The type of data visualization technique you leverage will vary based on the type of data you’re working with, in addition to the story you’re telling with your data.

<h3 id="dataVisualizationtopic-2">1. pie chart</h3>
<img class="lazy loaded" data-src="https://miro.medium.com/max/875/0*umJ0EqNjN1frc1Qo.png" src="https://miro.medium.com/max/875/0*umJ0EqNjN1frc1Qo.png" data-was-processed="true">

Pie charts are one of the most common and basic data visualization techniques, used across a wide range of applications. 
Pie charts are ideal for illustrating proportions, or part-to-whole comparisons.

Because pie charts are relatively simple and easy to read, they’re best suited for audiences who might be unfamiliar with the information or are only interested in the key takeaways. 
For viewers who require a more thorough explanation of the data, pie charts fall short in their ability to display complex information.
<h3 id="dataVisualizationtopic-3">2. bar chart</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*dFWTw-lVX3Nu7983.png">

The classic bar chart, or bar graph, is another common and easy-to-use method of data visualization. 
In this type of visualization, one axis of the chart shows the categories being compared, and the other, a measured value. 
The length of the bar indicates how each group measures according to the value.

One drawback is that labeling and clarity can become problematic when there are too many categories included. 
Like pie charts, they can also be too simple for more complex data sets.
<h3 id="dataVisualizationtopic-4">3. histogram</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*Z5grQpxQEXa-HsZn.png">

Unlike bar charts, histograms illustrate the distribution of data over a continuous interval or defined period. 
These visualizations are helpful in identifying where values are concentrated, as well as where there are gaps or unusual values.

Histograms are especially useful for showing the frequency of a particular occurrence. 
For instance, if you’d like to show how many clicks your website received each day over the last week, you can use a histogram. 
From this visualization, you can quickly determine which days your website saw the greatest and fewest number of clicks.
<h3 id="dataVisualizationtopic-5">4. gantt chart</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*Oqzn-95oz_8pc7TW.jpg">

Gantt charts are particularly common in project management, as they’re useful in illustrating a project timeline or progression of tasks. 
In this type of chart, tasks to be performed are listed on the vertical axis and time intervals on the horizontal axis. 
Horizontal bars in the body of the chart represent the duration of each activity.

Utilizing Gantt charts to display timelines can be incredibly helpful, and enable team members to keep track of every aspect of a project. 
Even if you’re not a project management professional, familiarizing yourself with Gantt charts can help you stay organized.
<h3 id="dataVisualizationtopic-6">5. heat map</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*5_715Jhi96_cK9jU.PNG">

A heat map is a type of visualization used to show differences in data through variations in color. 
These charts use color to communicate values in a way that makes it easy for the viewer to quickly identify trends. 
Having a clear legend is necessary in order for a user to successfully read and interpret a heatmap.

There are many possible applications of heat maps. 
For example, if you want to analyze which time of day a retail store makes the most sales, you can use a heat map that shows the day of the week on the vertical axis and time of day on the horizontal axis. 
Then, by shading in the matrix with colors that correspond to the number of sales at each time of day, you can identify trends in the data that allow you to determine the exact times your store experiences the most sales.
<h3 id="dataVisualizationtopic-7">6. a box and whisker plot</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*LIjHE91f2H-Cy_Qc.png">

A box and whisker plot, or box plot, provides a visual summary of data through its quartiles. 
First, a box is drawn from the first quartile to the third of the data set. 
A line within the box represents the median. 
“Whiskers,” or lines, are then drawn extending from the box to the minimum (lower extreme) and maximum (upper extreme). 
Outliers are represented by individual points that are in-line with the whiskers.

This type of chart is helpful in quickly identifying whether or not the data is symmetrical or skewed, as well as providing a visual summary of the data set that can be easily interpreted.
<h3 id="dataVisualizationtopic-8">7. waterfall chart</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*oOYFeAov5MkcNvjA.png">

A waterfall chart is a visual representation that illustrates how a value changes as it’s influenced by different factors, such as time. 
The main goal of this chart is to show the viewer how a value has grown or declined over a defined period. 
For example, waterfall charts are popular for showing spending or earnings over time.
<h3 id="dataVisualizationtopic-9">8. area chart</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*5wzIFNQWbOMNtP7W.png">

An area chart, or area graph, is a variation on a basic line graph in which the area underneath the line is shaded to represent the total value of each data point. 
When several data series must be compared on the same graph, stacked area charts are used.

This method of data visualization is useful for showing changes in one or more quantities over time, as well as showing how each quantity combines to make up the whole. 
Stacked area charts are effective in showing part-to-whole comparisons.
<h3 id="dataVisualizationtopic-10">9. scatter plot</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*VSS_E6MFexoM-2xj.png">

Another technique commonly used to display data is a scatter plot. 
A scatter plot displays data for two variables as represented by points plotted against the horizontal and vertical axis. 
This type of data visualization is useful in illustrating the relationships that exist between variables and can be used to identify trends or correlations in data.

Scatter plots are most effective for fairly large data sets, since it’s often easier to identify trends when there are more data points present. 
Additionally, the closer the data points are grouped together, the stronger the correlation or trend tends to be.
<h3 id="dataVisualizationtopic-11">10. pictogram chart</h3>
<img class="lazy" data-src="https://miro.medium.com/max/701/0*8ohJZyVjb75_DKae.PNG">

Pictogram charts, or pictograph charts, are particularly useful for presenting simple data in a more visual and engaging way. 
These charts use icons to visualize data, with each icon representing a different value or category. 
For example, data about time might be represented by icons of clocks or watches. 
Each icon can correspond to either a single unit or a set number of units (for example, each icon represents 100 units).

In addition to making the data more engaging, pictogram charts are helpful in situations where language or cultural differences might be a barrier to the audience’s understanding of the data.
<h3 id="dataVisualizationtopic-12">11. timeline</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*xb17-iirtOqh7OJh.jpg">

Timelines are the most effective way to visualize a sequence of events in chronological order. 
They’re typically linear, with key events outlined along the axis. 
Timelines are used to communicate time-related information and display historical data.

Timelines allow you to highlight the most important events that occurred, or need to occur in the future, and make it easy for the viewer to identify any patterns appearing within the selected time period. 
While timelines are often relatively simple linear visualizations, they can be made more visually appealing by adding images, colors, fonts, and decorative shapes.
<h3 id="dataVisualizationtopic-13">12. highlight table</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*q_bGsYvZYGq1hl70.jpg">

A highlight table is a more engaging alternative to traditional tables. 
By highlighting cells in the table with color, you can make it easier for viewers to quickly spot trends and patterns in the data. 
These visualizations are useful for comparing categorical data.

Depending on the data visualization tool you’re using, you may be able to add conditional formatting rules to the table that automatically color cells that meet specified conditions. 
For instance, when using a highlight table to visualize a company’s sales data, you may color cells red if the sales data is below the goal, or green if sales were above the goal. 
Unlike a heat map, the colors in a highlight table are discrete and represent a single meaning or value.
<h3 id="dataVisualizationtopic-14">13. bullet graph</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*mCFh-GlAxL4qJs9l.jpg">

A bullet graph is a variation of a bar graph that can act as an alternative to dashboard gauges to represent performance data. 
<mark>The main use for a bullet graph is to inform the viewer of how a business is performing in comparison to benchmarks that are in place for key business metrics.</mark>

In a bullet graph, the darker horizontal bar in the middle of the chart represents the actual value, while the vertical line represents a comparative value, or target. 
If the horizontal bar passes the vertical line, the target for that metric has been surpassed. 
Additionally, the segmented colored sections behind the horizontal bar represent range scores, such as “poor,” “fair,” or “good.”
<h3 id="dataVisualizationtopic-15">14. choropleth maps</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*zjRzCCpvme_Hq6Ma.jpg">

A choropleth map uses color, shading, and other patterns to visualize numerical values across geographic regions. 
These visualizations use a progression of color (or shading) on a spectrum to distinguish high values from low.

Choropleth maps allow viewers to see how a variable changes from one region to the next. 
A potential downside to this type of visualization is that the exact numerical values aren’t easily accessible because the colors represent a range of values. 
Some data visualization tools, however, allow you to add interactivity to your map so the exact values are accessible.
<h3 id="dataVisualizationtopic-16">15. word cloud</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*uLFYbDV8tcSctdXL.jpg">

A word cloud, or tag cloud, is a visual representation of text data in which the size of the word is proportional to its frequency. 
The more often a specific word appears in a dataset, the larger it appears in the visualization. 
In addition to size, words often appear bolder or follow a specific color scheme depending on their frequency.

Word clouds are often used on websites and blogs to identify significant keywords and compare differences in textual data between two sources. 
They are also useful when analyzing qualitative datasets, such as the specific words consumers used to describe a product.
<h3 id="dataVisualizationtopic-17">16. network diagram</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*KOsgOx84Si82Sygd.png">

Network diagrams are a type of data visualization that represent relationships between qualitative data points. 
These visualizations are composed of nodes and links, also called edges. 
Nodes are singular data points that are connected to other nodes through edges, which show the relationship between multiple nodes.

There are many use cases for network diagrams, including depicting social networks, highlighting the relationships between employees at an organization, or visualizing product sales across geographic regions.
<h3 id="dataVisualizationtopic-18">17. correlation matrix</h3>
<img class="lazy" data-src="https://miro.medium.com/max/875/0*2eEc0EZAnwrztj3p.png">

A correlation matrix is a table that shows correlation coefficients between variables. 
Each cell represents the relationship between two variables, and a color scale is used to communicate whether the variables are correlated and to what extent.

Correlation matrices are useful to summarize and find patterns in large data sets. 
In business, a correlation matrix might be used to analyze how different data points about a specific product might be related, such as price, advertising spend, launch date, etc.
<h3 id="dataVisualizationtopic-19">other data visualization options</h3>
While the examples listed above are some of the most commonly used techniques, there are many other ways you can visualize data to become a more effective communicator. 
Some other data visualization options include:

Bubble clouds
Cartograms
Circle views
Dendrograms
Dot distribution maps
Open-high-low-close charts
Polar areas
Radial trees
Ring Charts
Sankey diagram
Span charts
Streamgraphs
Treemaps
Wedge stack graphs
Violin plots
<h3 id="dataVisualizationtopic-20">tips for creating effective visualizations</h3>
Creating effective data visualizations requires more than just knowing how to choose the best technique for your needs. 
There are several considerations you should take into account to maximize your effectiveness when it comes to presenting data.

One of the most important steps is to evaluate your audience. 
For example, if you’re presenting financial data to a team that works in an unrelated department, you’ll want to choose a fairly simple illustration. 
On the other hand, if you’re presenting financial data to a team of finance experts, it’s likely you can safely include more complex information.

Another helpful tip is to avoid unnecessary distractions. 
Although visual elements like animation can be a great way to add interest, they can also distract from the key points the illustration is trying to convey and hinder the viewer’s ability to quickly understand the information.

Finally, be mindful of the colors you utilize, as well as your overall design. 
While it’s important that your graphs or charts are visually appealing, there are more practical reasons you might choose one color palette over another. 
For instance, using low contrast colors can make it difficult for your audience to discern differences between data points. 
Using colors that are too bold, however, can make the illustration overwhelming or distracting for the viewer.
<h3 id="dataVisualizationtopic-21">visuals to interpret and share information</h3>
No matter your role or title within an organization, data visualization is a skill that’s important for all professionals. 
Being able to effectively present complex data through easy-to-understand visual representations is invaluable when it comes to communicating information with members both inside and outside your business.

There’s no shortage in how data visualization can be applied in the real world. 
Data is playing an increasingly important role in the marketplace today, and data literacy is the first step in understanding how analytics can be used in business. 


<h2>K-Means Clustering in R Programming</h2>

K Means Clustering in R Programming is an Unsupervised Non-linear algorithm that cluster data based on similarity or similar groups. It seeks to partition the observations into a pre-specified number of clusters. Segmentation of data takes place to assign each training example to a segment called a cluster. In the unsupervised algorithm, high reliance on raw data is given with large expenditure on manual review for review of relevance is given. It is used in a variety of fields like Banking, healthcare, retail, Media, etc.
<h4>Theory</h4>
K-Means clustering groups the data on similar groups. The algorithm is as follows:
Choose the number <b>K</b> clusters.
Select at random K points, the centroids(Not necessarily from the given data).
Assign each data point to closest centroid that forms K clusters.
Compute and place the new centroid of each centroid.
Reassign each data point to new cluster.

After final reassignment, name the cluster as Final cluster.
<h4>The Dataset</h4>
<b><code>Iris</code></b> dataset consists of 50 samples from each of 3 species of Iris(Iris setosa, Iris virginica, Iris versicolor) and a multivariate dataset introduced by British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems. Four features were measured from each sample i.e length and width of the sepals and petals and based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.

<code># Loading data
data(iris)

# Structure 
str(iris)</code>

<h4>Performing K-Means Clustering on Dataset</h4>
Using K-Means Clustering algorithm on the dataset which includes 11 persons and 6 variables or attributes

<code># Installing Packages
install.packages("ClusterR")
install.packages("cluster")

# Loading package
library(ClusterR)
library(cluster)

# Removing initial label of 
# Species from original dataset
iris_1 &lt;- iris[, -5]

# Fitting K-Means clustering Model 
# to training dataset
set.seed(240) # Setting seed
kmeans.re &lt;- kmeans(iris_1, centers = 3, nstart = 20)
kmeans.re

# Cluster identification for 
# each observation
kmeans.re$cluster

# Confusion Matrix
cm &lt;- table(iris$Species, kmeans.re$cluster)
cm

# Model Evaluation and visualization
plot(iris_1[c("Sepal.Length", "Sepal.Width")])
plot(iris_1[c("Sepal.Length", "Sepal.Width")], 
     col = kmeans.re$cluster)
plot(iris_1[c("Sepal.Length", "Sepal.Width")], 
     col = kmeans.re$cluster, 
     main = "K-means with 3 clusters")

## Plotiing cluster centers
kmeans.re$centers
kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")]

# cex is font size, pch is symbol
points(kmeans.re$centers[, c("Sepal.Length", "Sepal.Width")], 
       col = 1:3, pch = 8, cex = 3) 

## Visualizing clusters
y_kmeans &lt;- kmeans.re$cluster
clusplot(iris_1[, c("Sepal.Length", "Sepal.Width")],
         y_kmeans,
         lines = 0,
         shade = TRUE,
         color = TRUE,
         labels = 2,
         plotchar = FALSE,
         span = TRUE,
         main = paste("Cluster iris"),
         xlab = 'Sepal.Length',
         ylab = 'Sepal.Width')</code>

<b>Output:</b>
<b>Model kmeans_re:</b>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20200617025946/Capture552.png">

The 3 clusters are made which are of 50, 62, and 38 sizes respectively. Within the cluster, the sum of squares is 88.4%.

<b>Cluster identification:</b>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20200617030059/Capture-1105.png">

The model achieved an accuracy of 100% with a p-value of less than 1. This indicates the model is good.

<b>Confusion Matrix:</b>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20200617030139/Capture_22.png">

So, 50 Setosa are correctly classified as Setosa. Out of 62 Versicolor, 48 Versicolor are correctly classified as Versicolor and 14 are classified as virginica. Out of 36 virginica, 19 virginica are correctly classified as virginica and 2 are classified as Versicolor.

<b>K-means with 3 clusters plot:</b><br><img src="https://media.geeksforgeeks.org/wp-content/uploads/20200617030334/capture_33.png">
The model showed 3 cluster plots with three different colors and with Sepal.length and with Sepal.width.

<b>Plotting cluster centers:</b>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20200617030506/Capture_4.png">

In the plot, centers of clusters are marked with cross signs with the same color of the cluster.

<b>Plot of clusters:</b>
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20200617182045/Capture_6.png">

So, 3 clusters are formed with varying sepal length and sepal width. Hence, the K-Means clustering algorithm is widely used in the industry.


<h2>K-Means Clustering in R: Step-by-Step Example</h2>
Clustering is a technique in machine learning that attempts to find <em>clusters</em> of observations within a dataset.
The goal is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.

Clustering is a form of unsupervised learning because we&#8217;re simply attempting to find structure within a dataset rather than predicting the value of some response variable.
Clustering is often used in marketing when companies have access to information like:

Household income
Household size
Head of household Occupation
Distance from nearest urban area

When this information is available, clustering can be used to identify households that are similar and may be more likely to purchase certain products or respond better to a certain type of advertising.

One of the most common forms of clustering is known as<b>k-means clustering</b>.
<h3>What is K-Means Clustering?</h3>
K-means clustering is a technique in which we place each observation in a dataset into one of <em>K</em> clusters.

The end goal is to have<em>K</em>clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other.
In practice, we use the following steps to perform K-means clustering:
<b>1. Choose a value for<em>K</em>.</b>
First, we must decide how many clusters we&#8217;d like to identify in the data. 
Often we have to simply test several different values for <em>K</em> and analyze the results to see which number of clusters seems to make the most sense for a given problem.

<b>2. Randomly assign each observation to an initial cluster, from 1 to<em>K.</em></b>
<b>3. Perform the following procedure until the cluster assignments stop changing.</b>

For each of the<em>K</em>clusters, compute the cluster<em>centroid.</em> This is simply the vector of the <em>p</em> feature means for the observations in the <em>k</em>th cluster.
Assign each observation to the cluster whose centroid is closest. 

Here,<em>closest</em> is defined using <a href="https://en.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance" target="_blank" rel="noopener noreferrer">Euclidean distance</a>.

<h3>K-Means Clustering in R</h3>
The following tutorial provides a step-by-step example of how to perform k-means clustering in R.

<h3>Step 1: Load the Necessary Packages</h3>
First, we&#8217;ll load two packages that contain several useful functions for k-means clustering in R.

<b>library(factoextra)
library(cluster)</b>

<h3>Step 2: Load and Prep the Data</h3>
For this example we&#8217;lluse the <em>USArrests</em>dataset built into R, which contains the number of arrests per 100,000 residents in each U.S. 
state in 1973 for <em>Murder</em>, <em>Assault</em>, and <em>Rape </em>along with the percentage of the population in each state living in urban areas, <em>UrbanPop</em>.

The following code shows how to do the following:
Load the <em>USArrests</em> dataset
Remove any rows with missing values

Scale each variable in the dataset to have a mean of 0 and a standard deviation of 1

<b>#load data
df &lt;- USArrests
#remove rows with missing values</b>
<b>df &lt;- na.omit(df)

#scale each variable to have a mean of 0 and sd of 1</b>
df &lt;- scale(df)
#view first six rows of dataset
head(df)
Murder   Assault   UrbanPop         Rape
Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
Arizona    0.07163341 1.4788032  0.9989801  1.042878388
Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
California 0.27826823 1.2628144  1.7589234  2.067820292
Colorado   0.02571456 0.3988593  0.8608085  1.864967207

<h3>Step 3: Find the Optimal Number of Clusters</h3>

To perform k-means clustering in R we can use the built-in <b>kmeans()</b> function, which uses the following syntax:
<b>kmeans(data, centers, nstart)</b>
where:
<b>data:</b> Name of the dataset.
<b>centers:</b> The number of clusters, denoted <em>k</em>.
<b>nstart:</b> The number of initial configurations. 
Because it&#8217;s possible that different initial starting clusters can lead to different results, it&#8217;s recommended to use several different initial configurations. 

The k-means algorithm will find the initial configurations that lead to the smallest within-cluster variation.

Since we don&#8217;t know beforehand how many clusters is optimal, we&#8217;ll create two different plots that can help us decide:

<b>1. Number of Clusters vs. the Total Within Sum of Squares</b>

First, we&#8217;ll use the<b>fviz_nbclust()</b> function to create a plot of the number of clusters vs. 
the total within sum of squares:
<b>fviz_nbclust(df, kmeans, method = "wss")</b>

<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmean1.png">
Typically when we create this type of plot we look for an &#8220;elbow&#8221; where the sum of squares begins to &#8220;bend&#8221; or level off. 
This is typically the optimal number of clusters.

For this plot it appear that there is a bit of an elbow or &#8220;bend&#8221; at k = 4 clusters.
<b>2. Number of Clusters vs. Gap Statistic</b>
Another way to determine the optimal number of clusters is to use a metric known as the <a href="http://web.stanford.edu/~hastie/Papers/gap.pdf" target="_blank" rel="noopener noreferrer">gap statistic</a>, which compares the total intra-cluster variation for different values of k with their expected values for a distribution with no clustering.
We can calculate the gap statistic for each number of clusters using the<b>clusGap()</b> function from the<em>cluster</em> package along with a plot of clusters vs. 

gap statistic using the <b>fviz_gap_stat()</b> function:
<b>#calculate gap statistic based on number of clusters
gap_stat &lt;- clusGap(df, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
#plot number of clusters vs. gap statistic
fviz_gap_stat(gap_stat)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmean2.png">
From the plot we can see that gap statistic is highest at k = 4 clusters, which matches the elbow method we used earlier.

<h3>Step 4: Perform K-Means Clustering with Optimal <em>K</em></h3>

Lastly, we can perform k-means clustering on the dataset using the optimal value for<em>k</em> of 4:

#make this example reproducible
set.seed(1)
#perform k-means clustering with k = 4 clusters
km &lt;- kmeans(df, centers = 4, nstart = 25)

#view results
km
K-means clustering with 4 clusters of sizes 16, 13, 13, 8

Cluster means:
Murder    Assault   UrbanPop        Rape
1 -0.4894375 -0.3826001  0.5758298 -0.26165379
2 -0.9615407 -1.1066010 -0.9301069 -0.96676331
3  0.6950701  1.0394414  0.7226370  1.27693964
4  1.4118898  0.8743346 -0.8145211  0.01927104

Clustering vector:
Alabama         Alaska        Arizona       Arkansas     California       Colorado 
4              3              3              4              3              3 

Connecticut       Delaware        Florida        Georgia         Hawaii          Idaho 
1              1              3              4              1              2 
Illinois        Indiana           Iowa         Kansas       Kentucky      Louisiana 

3              1              2              1              2              4 
Maine       Maryland  Massachusetts       Michigan      Minnesota    Mississippi 
2              3              1              3              2              4 

Missouri        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
3              2              2              3              2              1 
New Mexico       New York North Carolina   North Dakota           Ohio       Oklahoma 
3              3              4              2              1              1 
Oregon   Pennsylvania   Rhode Island South Carolina   South Dakota      Tennessee 
1              1              1              4              2              4 
Texas           Utah        Vermont       Virginia     Washington  West Virginia 
3              1              2              1              1              2 
Wisconsin        Wyoming 
2              1 
Within cluster sum of squares by cluster:
[1] 16.212213 11.952463 19.922437  8.316061

(between_SS / total_SS =  71.2 %)
Available components:
[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"   

[7] "size"         "iter"         "ifault"         

From the results we can see that:
<b>16 </b>states were assigned to the first cluster
<b>13</b> states were assigned to the second cluster
<b>13</b> states were assigned to the third cluster
<b>8 </b>states were assigned to the fourth cluster
We can visualize the clusters on a scatterplot that displays the first two principal components on the axes using the<b>fivz_cluster()</b> function:
<b>#plot results of final k-means model
fviz_cluster(km, data = df)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmean4.png">
We can also use the<b>aggregate()</b> function to find the mean of the variables in each cluster:

#find means of each cluster
aggregate(USArrests, by=list(cluster=km$cluster), mean)
cluster	  Murder   Assault	UrbanPop	    Rape
1	3.60000	  78.53846	52.07692	12.17692
2	10.81538 257.38462	76.00000	33.19231
3	5.65625	 138.87500	73.87500	18.78125
4	13.93750 243.62500	53.75000	21.41250

We interpret this output is as follows:
The mean number of murders per 100,000 citizens among the states in cluster 1 is <b>3.6</b>.
The mean number of assaults per 100,000 citizens among the states in cluster 1 is <b>78.5</b>.

The mean percentage of residents living in an urban area among the states in cluster 1 is <b>52.1%</b>.
The mean number of rapes per 100,000 citizens among the states in cluster 1 is <b>12.2</b><b>.</b>
And so on.
We can also append the cluster assignments of each state back to the original dataset:
<b>#add cluster assigment to original data

final_data &lt;- cbind(USArrests, cluster = km$cluster)
#view final data
head(final_data)

Murder	Assault	UrbanPop  Rape	 cluster
Alabama	    13.2	236	58	  21.2	 4
Alaska	    10.0	263	48	  44.5	 2
Arizona	     8.1	294	80	  31.0	 2
Arkansas     8.8	190	50	  19.5	 4
California   9.0	276	91	  40.6	 2
Colorado     7.9	204	78	  38.7	 2
</b>
<h3>Pros &amp; Cons of K-Means Clustering</h3>
K-means clustering offers the following benefits:

It is a fast algorithm.

It can handle large datasets well.

However, it comes with the following potential drawbacks:
It requires us to specify the number of clusters before performing the algorithm.
It&#8217;s sensitive to outliers.
Two alternatives to k-means clustering are <a href="https://www.statology.org/k-medoids-in-r/" target="_blank" rel="noopener noreferrer">k-medoids clustering</a> and <a href="https://www.statology.org/hierarchical-clustering-in-r/" target="_blank" rel="noopener noreferrer">hierarchical clustering</a>.
You can find the complete R code used in this example <a href="https://github.com/Statology/R-Guides/blob/main/k_means.R" target="_blank" rel="noopener noreferrer">here</a>.

<h2>Chrome Devtools Protocol</h2>
https://github.com/rstudio/chromote
Chromote is <b>not</b> the only R package that implements the Chrome Devtools Protocol.

Here are some others:
crrri by Romain Lesur and Christophe Dervieux
decapitated by Bob Rudis
chradle by Miles McBain

Installation
remotes::install_github("rstudio/chromote")







<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
randomScroll();
</script>
</pre></body></html>
